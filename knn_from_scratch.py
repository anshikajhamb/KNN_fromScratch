# -*- coding: utf-8 -*-
"""KNN_from_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/172wZJLnH72sIsvjj9BJFpS4Izq39tC0C

KNN Algorithm:

Step 1 − For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.

 1.1 - Normalize the data points. Divide them into training and test dataset 

Step 2 − Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.

Step 3 − For each point in the test data do the following −

3.1 − Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.

3.2 − Now, based on the distance value, sort them in ascending order.

3.3 − Next, it will choose the top K rows from the sorted array.

3.4 − Now, it will assign a class to the test point based on most frequent class of these rows.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math
import seaborn as sns

path = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
headernames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']
dataset = pd.read_csv(path, names = headernames)
dataset.head(n=100)

from sklearn.model_selection import train_test_split

x = dataset.iloc[:, [1,3]].values #column 1 (sepal length) and 3 (petal length) are our 2 dimensions/features 
y = dataset.iloc[:, 4].values

#normalizing testing and training dataset 
col1 = x[:, 0]
col1_min = min(col1)
col1_max = max(col1)
col2 = x[:, 1]
col2_min = min(col2)
col2_max = max(col2)

range_col1 = col1_max - col1_min
range_col2 = col2_max - col2_min

norm_x = [] #storing normalized data

for i in range(len(col1)):
  norm_x.append([(x[i][0]-col1_min)/(range_col1), (x[i][1]-col2_min)/(range_col2)])


#splitting data set such that 30% is attributed to test whereas 70% is attributed to training.
x_train, x_test, y_train, y_test = train_test_split(norm_x, y, test_size = 0.30)

""" Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.

Now, let's begin the main body of KNN Algorithm. 
"""

# finding Euclidean Distance 
def EucledianDist(x, y): 
  euc_dist = (math.sqrt(pow((x[0]-y[0]),2) + pow((x[1]-y[1]),2)))
  return euc_dist


#find most common element in kClassifications
def most_common(List):
  counter = 0
  classification = List[0]
     
  for i in List:
      curr_frequency = List.count(i)
      if(curr_frequency> counter):
          counter = curr_frequency
          classification = i
 
  return classification

k = 7 #defining k. recommended to be an odd number. 
EucledianDistanceList = [] # storing all eucledian distances
kNeighbours = []
kClassifications = []

for train in range(len(x_train)):
  for test in range(len(x_test)):
    eucledianDistance = EucledianDist(x_train[train], x_test[test])
    EucledianDistanceList.append([eucledianDistance, y_test[test]])
    EucledianDistanceList.sort()

  kNeighbours = EucledianDistanceList[:k] # list with first k shortest distances from the training datapoint. 
  for n in range(len(kNeighbours)):
    kClassifications.append(kNeighbours[n][1])
  training_set_classification = most_common(kClassifications)
  print('training data:', x_train[train], 'classification:', training_set_classification) # training data classified based on k nearest test datapoints
  
  kNeighbours = []
  EucledianDistanceList= []
  kClassifications = []

x_train_df = pd.DataFrame(x_train,columns=['x','y'])
x_train_df['class'] = y_train


x_test_df = pd.DataFrame(x_test,columns=['x','y'])
x_test_df['class'] = y_test

plt.scatter(x_train_df.loc[(x_train_df['class']=='Iris-setosa')]['x'],
            x_train_df.loc[(x_train_df['class']=='Iris-setosa')]['y'],color='black',label='Iris-setosa');
plt.scatter(x_train_df.loc[(x_train_df['class']=='Iris-versicolor')]['x'],
            x_train_df.loc[(x_train_df['class']=='Iris-versicolor')]['y'],color='blue',label='Iris-versicolor');
plt.scatter(x_train_df.loc[(x_train_df['class']=='Iris-virginica')]['x'],
            x_train_df.loc[(x_train_df['class']=='Iris-virginica')]['y'],color='orange',label='Iris-virginica');
plt.legend()
plt.xlabel("sepal-length")
plt.ylabel("petal-length")
plt.title("Training set classifications")
plt.show();

plt.scatter(x_test_df.loc[(x_test_df['class']=='Iris-setosa')]['x'],
            x_test_df.loc[(x_test_df['class']=='Iris-setosa')]['y'],color='black',label='Iris-setosa');
plt.scatter(x_test_df.loc[(x_test_df['class']=='Iris-versicolor')]['x'],
            x_test_df.loc[(x_test_df['class']=='Iris-versicolor')]['y'],color='blue',label='Iris-versicolor');
plt.scatter(x_test_df.loc[(x_test_df['class']=='Iris-virginica')]['x'],
            x_test_df.loc[(x_test_df['class']=='Iris-virginica')]['y'],color='orange',label='Iris-virginica');
plt.legend()
plt.xlabel("sepal-length")
plt.ylabel("petal-length")
plt.title("Testing set classifications");